# XAI and Privacy

Explainable AI (XAI) techniques provide a way to build user trust in AI systems. XAI comes in part from an effort by researches to build ethical AI systems or "Trustworthy AI".
 Trustworthy AI as a whole encompasses work in privacy and security; explainability and interpretability; fairness; and robustness.

I have started looking at ways to make privacy preserving techniques for AI (e.g., differential privacy and federated learning) more transparent
to users by leveraging XAI techniques. This could allow us to better understand how privacy techniques affect model behavior. I hope this work allows us to build more transparent, secure AI models.

This idea is supported by a belief that privacy solutions for AI look different based on sector application and user demographic. Popular privacy solutions still seem to operate under a "one size fits all" framework.

+# AI Privacy Policy

I have partnered with some wonderful people in the public policy department to examine how privacy is affected by AI in the state of Arkansas.

I have long wanted to understand how local and national policy could be leveraged to build safer, more equitable technology.
Clear themes emerged by the time I was finishing my undergraduate degree: technology affects some demographic groups negatively; safety and privacy are not always prioritized by developers;
and technical jargon can obscure very important messages for users.

All of those themes appear in research on privacy, trustworthy, and ethical AI. It is an interdisciplinary
conversation being tackled from multiple points of view. It means a great deal to me that technical research informs conversations on policy and ethics.





